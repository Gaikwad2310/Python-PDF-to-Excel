import numpy as np
import pandas as pd
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

# Define the better IV model (tanh + quadratic)
def better_iv_model(k, alpha, beta, gamma, delta, m):
    return alpha + beta * np.tanh(gamma * (k - m)) + delta * (k - m)**2

# Step 1: Fit the IV smile for each maturity
def fit_better_iv_model_with_params(df, spot_price):
    df = df.copy()
    df["k"] = np.log(df["K"] / spot_price)
    df["w"] = (df["iv"] ** 2) * df["T"]
    results = []

    for T_val in sorted(df["T"].unique()):
        sub_df = df[df["T"] == T_val]
        k = sub_df["k"].values
        w = sub_df["w"].values

        alpha_0 = np.mean(w)
        beta_0 = 0.1
        gamma_0 = 5.0
        delta_0 = 0.1
        m_0 = np.mean(k)
        p0 = [alpha_0, beta_0, gamma_0, delta_0, m_0]

        bounds = ([-10, -10, 0.1, 0, min(k) - 1],
                  [10, 10, 50, 10, max(k) + 1])

        try:
            popt, _ = curve_fit(better_iv_model, k, w, p0=p0, bounds=bounds, maxfev=5000)
            alpha, beta, gamma, delta, m = popt
            w_fit = better_iv_model(k, alpha, beta, gamma, delta, m)
            iv_fit = np.sqrt(np.maximum(w_fit / T_val, 0))

            sub_df = sub_df.copy()
            sub_df["w_fit"] = w_fit
            sub_df["iv_fit"] = iv_fit
            sub_df["alpha"] = alpha
            sub_df["beta"] = beta
            sub_df["gamma"] = gamma
            sub_df["delta"] = delta
            sub_df["m"] = m

            results.append(sub_df)
        except RuntimeError as e:
            print(f"Fitting failed for T={T_val}: {e}")

    return pd.concat(results, ignore_index=True)

# Step 2: Compute local volatility using Dupire formula
def compute_local_volatility(df, spot_price):
    df = df.copy()
    df["k"] = np.log(df["K"] / spot_price)
    df["w"] = df["iv_fit"] ** 2 * df["T"]

    df["dw_dT"] = np.nan
    df["dw_dk"] = np.nan
    df["d2w_dk2"] = np.nan
    df["local_vol"] = np.nan

    for T_val in sorted(df["T"].unique()):
        sub_df = df[df["T"] == T_val].copy()
        k = sub_df["k"].values
        w = sub_df["w"].values

        sort_idx = np.argsort(k)
        k_sorted = k[sort_idx]
        w_sorted = w[sort_idx]

        dw_dk = np.gradient(w_sorted, k_sorted)
        d2w_dk2 = np.gradient(dw_dk, k_sorted)

        T_eps = 1e-4
        T_up = T_val + T_eps
        T_down = T_val - T_eps

        df_T_up = sub_df.copy()
        df_T_down = sub_df.copy()

        df_T_up["w"] = df_T_up["iv_fit"] ** 2 * T_up
        df_T_down["w"] = df_T_down["iv_fit"] ** 2 * T_down

        dw_dT = (df_T_up["w"].values - df_T_down["w"].values) / (2 * T_eps)

        numerator = dw_dT
        denominator = (1 - k_sorted * dw_dk / w_sorted) ** 2 + 0.5 * d2w_dk2 * w_sorted

        with np.errstate(divide='ignore', invalid='ignore'):
            local_var = np.where(denominator > 0, numerator / denominator, np.nan)
            local_vol = np.sqrt(np.maximum(local_var, 0))

        idx = sub_df.index[sort_idx]
        df.loc[idx, "dw_dT"] = dw_dT
        df.loc[idx, "dw_dk"] = dw_dk
        df.loc[idx, "d2w_dk2"] = d2w_dk2
        df.loc[idx, "local_vol"] = local_vol

    return df

# Step 3: Plot fitted IV curve
def plot_smooth_fitted_iv_by_strike(df, spot_price):
    df = df.copy()
    unique_T = sorted(df["T"].unique())

    plt.figure(figsize=(12, 6))
    for T_val in unique_T:
        sub_df = df[df["T"] == T_val].iloc[0]
        alpha = sub_df["alpha"]
        beta = sub_df["beta"]
        gamma = sub_df["gamma"]
        delta = sub_df["delta"]
        m = sub_df["m"]

        K_min, K_max = df["K"].min(), df["K"].max()
        K_vals = np.linspace(K_min, K_max, 100)
        k_vals = np.log(K_vals / spot_price)

        w_fit = better_iv_model(k_vals, alpha, beta, gamma, delta, m)
        iv_fit = np.sqrt(np.maximum(w_fit / T_val, 0))

        K_market = df[df["T"] == T_val]["K"]
        iv_market = df[df["T"] == T_val]["iv"]

        plt.plot(K_vals, iv_fit, '-', label=f"Fitted IV (T={T_val})")
        plt.plot(K_market, iv_market, 'o', label=f"Market IV (T={T_val})")

    plt.xlabel("Strike Price (K)")
    plt.ylabel("Implied Volatility")
    plt.title("Fitted vs Market IV")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Sample data
extended_df = pd.DataFrame({
    "K": [9500, 9700, 9900, 10100, 10300, 10500]*2,
    "T": [0.2]*6 + [0.4]*6,
    "iv": [0.18, 0.17, 0.16, 0.165, 0.17, 0.175, 0.19, 0.18, 0.17, 0.172, 0.176, 0.18]
})
spot_price = 10000

# Run everything
fitted_df = fit_better_iv_model_with_params(extended_df, spot_price)
local_vol_df = compute_local_volatility(fitted_df, spot_price)
plot_smooth_fitted_iv_by_strike(fitted_df, spot_price)

# Define the new model: logistic-tanh + quadratic
def better_iv_model(k, alpha, beta, gamma, delta, m):
    return alpha + beta * np.tanh(gamma * (k - m)) + delta * (k - m)**2

# New function to fit this model using curve_fit
def fit_better_iv_model(df, spot_price):
    df = df.copy()
    df["k"] = np.log(df["K"] / spot_price)
    df["w"] = (df["iv"] ** 2) * df["T"]

    results = []

    for T_val in sorted(df["T"].unique()):
        sub_df = df[df["T"] == T_val]
        k = sub_df["k"].values
        w = sub_df["w"].values

        # Initial guess
        alpha_0 = np.mean(w)
        beta_0 = 0.1
        gamma_0 = 5.0
        delta_0 = 0.1
        m_0 = np.mean(k)

        p0 = [alpha_0, beta_0, gamma_0, delta_0, m_0]

        bounds = (
            [-10, -10, 0.1, 0, min(k) - 1],  # Lower bounds
            [10, 10, 50, 10, max(k) + 1]     # Upper bounds
        )

        try:
            popt, _ = curve_fit(
                better_iv_model, k, w, p0=p0, bounds=bounds, maxfev=5000
            )

            alpha, beta, gamma, delta, m = popt
            w_fit = better_iv_model(k, alpha, beta, gamma, delta, m)
            iv_fit = np.sqrt(np.maximum(w_fit / T_val, 0))

            sub_df = sub_df.copy()
            sub_df["w_fit"] = w_fit
            sub_df["iv_fit"] = iv_fit
            sub_df["alpha"] = alpha
            sub_df["beta"] = beta
            sub_df["gamma"] = gamma
            sub_df["delta"] = delta
            sub_df["m"] = m

            results.append(sub_df)
        except RuntimeError as e:
            print(f"Curve fitting failed for T={T_val}: {e}")

    return pd.concat(results, ignore_index=True)

# Apply the new better curve fitting model
fitted_better_model_df = fit_better_iv_model(extended_df, spot_price=10000)
tools.display_dataframe_to_user(name="Better Curve Fit (Logistic+Quadratic)", dataframe=fitted_better_model_df)

from scipy.optimize import curve_fit

# New function using curve_fit instead of minimize
def fit_svi_jumpwings_curvefit(df, spot_price):
    df = df.copy()
    df["k"] = np.log(df["K"] / spot_price)
    df["w"] = (df["iv"] ** 2) * df["T"]

    results = []

    for T_val in sorted(df["T"].unique()):
        sub_df = df[df["T"] == T_val]
        k = sub_df["k"].values
        w = sub_df["w"].values

        # Initial guess
        theta_0 = np.mean(w)
        phi_0 = 0.5
        rho_0 = -0.4
        kappa_0 = np.mean(k)

        p0 = [theta_0, phi_0, rho_0, kappa_0]

        bounds = (
            [1e-6, 1e-3, -0.999, min(k) - 1],  # Lower bounds
            [5, 10, 0.999, max(k) + 1]         # Upper bounds
        )

        try:
            popt, _ = curve_fit(
                svi_jumpwings, k, w, p0=p0, bounds=bounds, maxfev=5000
            )

            theta, phi, rho, kappa = popt
            w_svi = svi_jumpwings(k, theta, phi, rho, kappa)
            iv_svi = np.sqrt(np.maximum(w_svi / T_val, 0))  # Avoid sqrt of negative

            sub_df = sub_df.copy()
            sub_df["w_svi"] = w_svi
            sub_df["iv_svi"] = iv_svi
            sub_df["theta"] = theta
            sub_df["phi"] = phi
            sub_df["rho"] = rho
            sub_df["kappa"] = kappa

            results.append(sub_df)
        except RuntimeError as e:
            print(f"Curve fitting failed for T={T_val}: {e}")

    return pd.concat(results, ignore_index=True)

# Apply the new curve_fit-based SVI fitting
fitted_multi_df_curvefit = fit_svi_jumpwings_curvefit(extended_df, spot_price=10000)
tools.display_dataframe_to_user(name="Curve Fit SVI Multi-T", dataframe=fitted_multi_df_curvefit)

import numpy as np
import pandas as pd
from scipy.optimize import minimize

# SVI Jump-Wings parameterization function
def svi_jumpwings(k, theta, phi, rho, kappa):
    return theta * (1 + rho * phi * (k - kappa) + np.sqrt((phi * (k - kappa) + rho) ** 2 + 1 - rho ** 2))

# Objective function for least squares
def svi_loss(params, k, w_obs):
    theta, phi, rho, kappa = params
    w_model = svi_jumpwings(k, theta, phi, rho, kappa)
    return np.mean((w_model - w_obs) ** 2)

# Fit SVI Jump-Wings to data
def fit_svi_jumpwings(df, spot_price):
    df = df.copy()
    df["k"] = np.log(df["K"] / spot_price)
    df["w"] = (df["iv"] ** 2) * df["T"]

    k = df["k"].values
    w = df["w"].values

    # Initial guess and bounds
    theta_0 = np.mean(w)
    phi_0 = 0.1
    rho_0 = -0.5
    kappa_0 = 0.0

    bounds = [
        (1e-6, 5),     # theta
        (1e-6, 5),     # phi
        (-0.999, 0.999),  # rho
        (-1.0, 1.0)    # kappa
    ]

    result = minimize(
        svi_loss,
        x0=[theta_0, phi_0, rho_0, kappa_0],
        args=(k, w),
        bounds=bounds,
        method='L-BFGS-B'
    )

    if result.success:
        theta, phi, rho, kappa = result.x
        df["w_svi"] = svi_jumpwings(k, theta, phi, rho, kappa)
        df["iv_svi"] = np.sqrt(df["w_svi"] / df["T"])
        params = {"theta": theta, "phi": phi, "rho": rho, "kappa": kappa}
        return df, params
    else:
        raise RuntimeError("SVI fitting failed: " + result.message)

# Example input format
example_df = pd.DataFrame({
    "K": [9500, 9700, 9900, 10100, 10300, 10500],
    "T": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2],
    "iv": [0.18, 0.17, 0.16, 0.165, 0.17, 0.175]
})

spot_price = 10000
fitted_df, fitted_params = fit_svi_jumpwings(example_df, spot_price)
import ace_tools as tools; tools.display_dataframe_to_user(name="SVI Fitted Data", dataframe=fitted_df)
fitted_params

# Modify the previous code to handle multiple maturities T

def fit_svi_jumpwings_multiple_T(df, spot_price):
    df = df.copy()
    df["k"] = np.log(df["K"] / spot_price)
    df["w"] = (df["iv"] ** 2) * df["T"]
    
    results = []
    
    for T_val in sorted(df["T"].unique()):
        sub_df = df[df["T"] == T_val]
        k = sub_df["k"].values
        w = sub_df["w"].values

        # Initial guess and bounds
        theta_0 = np.mean(w)
        phi_0 = 0.1
        rho_0 = -0.5
        kappa_0 = 0.0

        bounds = [
            (1e-6, 5),     # theta
            (1e-6, 5),     # phi
            (-0.999, 0.999),  # rho
            (-1.0, 1.0)    # kappa
        ]

        result = minimize(
            svi_loss,
            x0=[theta_0, phi_0, rho_0, kappa_0],
            args=(k, w),
            bounds=bounds,
            method='L-BFGS-B'
        )

        if result.success:
            theta, phi, rho, kappa = result.x
            w_svi = svi_jumpwings(k, theta, phi, rho, kappa)
            iv_svi = np.sqrt(w_svi / T_val)

            sub_df = sub_df.copy()
            sub_df["w_svi"] = w_svi
            sub_df["iv_svi"] = iv_svi
            sub_df["theta"] = theta
            sub_df["phi"] = phi
            sub_df["rho"] = rho
            sub_df["kappa"] = kappa

            results.append(sub_df)
        else:
            print(f"SVI fitting failed for T={T_val}")

    return pd.concat(results, ignore_index=True)

# Extend the example DataFrame to include multiple T
extended_df = pd.DataFrame({
    "K": [9500, 9700, 9900, 10100, 10300, 10500]*2,
    "T": [0.2]*6 + [0.4]*6,
    "iv": [0.18, 0.17, 0.16, 0.165, 0.17, 0.175, 0.19, 0.18, 0.17, 0.172, 0.176, 0.18]
})

fitted_multi_df = fit_svi_jumpwings_multiple_T(extended_df, spot_price=10000)
tools.display_dataframe_to_user(name="SVI Multi-T Fit", dataframe=fitted_multi_df)


import numpy as np
import pandas as pd
from scipy.optimize import minimize

def svi_raw(k, a, b, rho, m, sigma):
    return a + b * (rho * (k - m) + np.sqrt((k - m)**2 + sigma**2))

def svi_objective(params, k, w):
    a, b, rho, m, sigma = params
    w_fit = svi_raw(k, a, b, rho, m, sigma)
    return np.mean((w - w_fit) ** 2)

def initial_guess_and_bounds(k, w):
    a_init = np.min(w)
    b_init = (np.max(w) - np.min(w)) / 2
    rho_init = 0.0
    m_init = np.mean(k)
    sigma_init = np.std(k) / 2

    guess = [a_init, b_init, rho_init, m_init, sigma_init]
    bounds = [
        (0, 2 * np.max(w)),         # a â‰¥ 0
        (1e-5, 10),                 # b > 0
        (-0.999, 0.999),           # -1 < rho < 1
        (np.min(k), np.max(k)),    # m in range of k
        (1e-5, 2 * np.std(k))      # sigma > 0
    ]
    return guess, bounds

def fit_svi_raw_all_expiries(df, spot_price, r=0.05):
    df = df.copy()
    svi_params = []

    for T, group in df.groupby('T'):
        group = group.copy()
        F = spot_price * np.exp(r * T)
        k = np.log(group['STRIKE'] / F)
        iv = group['implied_vol'].values
        w = (iv ** 2) * T

        guess, bounds = initial_guess_and_bounds(k, w)
        result = minimize(svi_objective, guess, args=(k, w), bounds=bounds, method='L-BFGS-B')

        if result.success:
            a, b, rho, m, sigma = result.x
            fitted_w = svi_raw(k, a, b, rho, m, sigma)
            fitted_iv = np.sqrt(fitted_w / T)
            abs_diff = np.abs(iv - fitted_iv)

            df.loc[group.index, 'log_moneyness'] = k
            df.loc[group.index, 'svi_total_variance'] = fitted_w
            df.loc[group.index, 'svi_iv'] = fitted_iv
            df.loc[group.index, 'svi_abs_error'] = abs_diff
            df.loc[group.index, 'a'] = a
            df.loc[group.index, 'b'] = b
            df.loc[group.index, 'rho'] = rho
            df.loc[group.index, 'm'] = m
            df.loc[group.index, 'sigma'] = sigma
            df.loc[group.index, 'svi_fit_error'] = result.fun
        else:
            # Fill with NaN if fitting fails
            df.loc[group.index, ['log_moneyness', 'svi_total_variance', 'svi_iv', 'svi_abs_error',
                                 'a', 'b', 'rho', 'm', 'sigma', 'svi_fit_error']] = np.nan

    return df.reset_index(drop=True)


import numpy as np
import pandas as pd
from scipy.signal import savgol_filter
from scipy.interpolate import interp1d

def smart_filter_for_svi(df, spot_price, r=0.05, t_col='T', k_col='STRIKE', iv_col='implied_vol', n_points=20):
    result = []
    
    for T, group in df.groupby(t_col):
        group = group.copy()
        
        # Forward price
        F = spot_price * np.exp(r * T)
        group['log_moneyness'] = np.log(group[k_col] / F)
        
        # Sort by moneyness
        group = group.sort_values('log_moneyness')
        
        # Apply smoothing filter
        window = min(11, len(group)) if len(group) % 2 == 1 else min(10, len(group)-1)
        if window < 5:
            result.append(group)
            continue
        
        try:
            smoothed_iv = savgol_filter(group[iv_col].values, window_length=window, polyorder=2)
        except:
            result.append(group)
            continue
        
        group['smoothed_iv'] = smoothed_iv
        group['iv_error'] = np.abs(group[iv_col] - group['smoothed_iv'])

        # Remove points with large deviation (spikes, noise)
        threshold = 2 * group['iv_error'].std()
        group = group[group['iv_error'] < threshold]

        # Re-sample evenly across moneyness
        if len(group) > n_points:
            idx = np.linspace(0, len(group) - 1, n_points, dtype=int)
            group = group.iloc[idx]
        
        result.append(group.drop(columns=['smoothed_iv', 'iv_error']))
    
    return pd.concat(result).reset_index(drop=True)

import pandas as pd
import numpy as np
from scipy.stats import zscore

def remove_outliers_iqr(group, iv_col='implied_vol'):
    Q1 = group[iv_col].quantile(0.25)
    Q3 = group[iv_col].quantile(0.75)
    IQR = Q3 - Q1
    return group[(group[iv_col] >= Q1 - 1.5 * IQR) & (group[iv_col] <= Q3 + 1.5 * IQR)]

def select_representative_points(group, k_col='STRIKE', n_points=10):
    group = group.sort_values(k_col)
    if len(group) <= n_points:
        return group
    return group.iloc[np.linspace(0, len(group)-1, n_points, dtype=int)]

def clean_iv_data(df, t_col='T', iv_col='implied_vol', k_col='STRIKE', n_points=10):
    cleaned_df = []
    for T_val, group in df.groupby(t_col):
        group = remove_outliers_iqr(group, iv_col)
        selected = select_representative_points(group, k_col, n_points)
        cleaned_df.append(selected)
    return pd.concat(cleaned_df).reset_index(drop=True)

import pandas as pd
import itertools

# Example: your dataframe has columns like this
# 'T', 'K', 'a', 'b', 'rho', 'm', 'sigma'

# Sample input
# df = pd.read_csv('your_data.csv')

# Extract unique (T, parameters) pairs
t_param_cols = ['T', 'a', 'b', 'rho', 'm', 'sigma']
unique_t_params = df[t_param_cols].drop_duplicates()

# Extract unique K values
unique_K = df['K'].unique()

# Create a DataFrame from cartesian product of K and T-parameter sets
result = pd.DataFrame([
    {**dict(row), 'K': k}
    for _, row in unique_t_params.iterrows()
    for k in unique_K
])

# Reorder columns if needed
result = result[['T', 'K', 'a', 'b', 'rho', 'm', 'sigma']]

print(result.head())

import numpy as np
import pandas as pd
from scipy.stats import norm
from scipy.optimize import minimize
from scipy.interpolate import UnivariateSpline, LinearNDInterpolator
from scipy.linalg import solve_banded

# ---- 1. Black-Scholes and Vega ----
def black_scholes_call_price(S, K, T, r, sigma):
    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T) + 1e-10)
    d2 = d1 - sigma * np.sqrt(T)
    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)

def vega(S, K, T, r, sigma):
    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T) + 1e-10)
    return S * norm.pdf(d1) * np.sqrt(T)

def newton_raphson_iv(market_price, S, K, T, r, tol=1e-6, max_iter=100):
    sigma = 0.2
    for _ in range(max_iter):
        price = black_scholes_call_price(S, K, T, r, sigma)
        v = vega(S, K, T, r, sigma)
        if v == 0:
            break
        diff = price - market_price
        sigma -= diff / v
        if abs(diff) < tol:
            return sigma
    return np.nan

# ---- 2. SVI Fits ----
def svi_raw(k, a, b, rho, m, sigma):
    return a + b * (rho * (k - m) + np.sqrt((k - m)**2 + sigma**2))

def svi_natural(k, a, b, rho, m, sigma):
    return a + b * np.tanh(rho * (k - m)) + sigma * np.abs(k - m)

def svi_jumpwings(k, a, b, rho, m, sigma):
    return svi_raw(k, a, b, rho, m, sigma)

def fit_svi(df, k, w):
    def obj(fn, params): return np.mean((w - fn(k, *params))**2)
    bounds = [(-1, 1), (1e-5, 10), (-0.999, 0.999), (-2, 2), (1e-5, 2)]
    initial = [0.1, 0.1, 0.0, 0.0, 0.1]

    res_raw = minimize(lambda p: obj(svi_raw, p), initial, bounds=bounds)
    res_nat = minimize(lambda p: obj(svi_natural, p), initial, bounds=bounds)
    res_jump = minimize(lambda p: obj(svi_jumpwings, p), initial, bounds=bounds)

    df["SVI_raw_fit"] = svi_raw(k, *res_raw.x)
    df["SVI_natural_fit"] = svi_natural(k, *res_nat.x)
    df["SVI_jumpwings_fit"] = svi_jumpwings(k, *res_jump.x)

    return df, {
        "raw": (res_raw.x, obj(svi_raw, res_raw.x)),
        "natural": (res_nat.x, obj(svi_natural, res_nat.x)),
        "jumpwings": (res_jump.x, obj(svi_jumpwings, res_jump.x)),
    }

# ---- 3. Arbitrage Checks ----
def butterfly_arbitrage_check(k, w_k):
    w_prime = np.gradient(w_k, k)
    w_double_prime = np.gradient(w_prime, k)
    g_k = ((1 - (k * w_prime) / (2 * w_k))**2
           - 0.25 * w_prime**2 * (1 / w_k + 0.25)
           + 0.5 * w_double_prime)
    return np.all(g_k >= -1e-5)

def calendar_arbitrage_check(df):
    grouped = df.groupby("T")
    maturities = sorted(grouped.groups.keys())
    for i in range(len(maturities) - 1):
        df1 = grouped.get_group(maturities[i])
        df2 = grouped.get_group(maturities[i+1])
        k_common = np.intersect1d(df1["log_moneyness"], df2["log_moneyness"])
        if len(k_common) == 0:
            continue
        w1 = np.interp(k_common, df1["log_moneyness"], df1["total_variance"])
        w2 = np.interp(k_common, df2["log_moneyness"], df2["total_variance"])
        if np.any(w2 < w1 - 1e-5):
            return False
    return True

# ---- 4. Dupire Local Vol ----
def compute_local_volatility(df, svi_column="SVI_raw_fit"):
    local_vol_list = []
    grouped = df.groupby("T")
    for T, df_T in grouped:
        df_T = df_T.sort_values("log_moneyness")
        k = df_T["log_moneyness"].values
        w = df_T[svi_column].values
        if len(k) < 5:
            df_T["LocalVol"] = np.nan
            local_vol_list.append(df_T)
            continue
        spline = UnivariateSpline(k, w, s=1e-10, k=4)
        dw_dk = spline.derivative(1)(k)
        d2w_dk2 = spline.derivative(2)(k)
        dw_dT = np.gradient(w, T)
        num = dw_dT
        denom = (1 - (k * dw_dk) / w + 0.25 * ((-0.25 - 1 / w + k**2 / w**2) * dw_dk**2) + 0.5 * d2w_dk2)
        sigma2 = np.maximum(num / denom, 1e-6)
        df_T["LocalVol"] = np.sqrt(sigma2)
        local_vol_list.append(df_T)
    return pd.concat(local_vol_list).sort_index()

# ---- 5. Crank-Nicolson PDE ----
def get_local_vol_interpolator(df):
    unique_T = df["T"].unique()
    if len(unique_T) == 1:
        df_T = df[df["T"] == unique_T[0]].sort_values("STRIKE")
        return lambda K, T: np.interp(K, df_T["STRIKE"], df_T["LocalVol"])
    else:
        pts = np.array([df["STRIKE"], df["T"]]).T
        return LinearNDInterpolator(pts, df["LocalVol"], fill_value=1e-4)

def crank_nicolson_local_vol(K, T, S0, r, lv_interp, K_grid, N=100, M=100):
    dt = T / M
    dK = K_grid[1] - K_grid[0]
    grid = np.maximum(K_grid - S0, 0)
    for _ in range(M):
        sigma = lv_interp(K_grid, T)
        sigma[np.isnan(sigma)] = 1e-4
        a = 0.25 * dt * (sigma**2 * K_grid**2 / dK**2 - r * K_grid / dK)
        b = -0.5 * dt * (sigma**2 * K_grid**2 / dK**2 + r)
        c = 0.25 * dt * (sigma**2 * K_grid**2 / dK**2 + r * K_grid / dK)
        lower = -a[2:N-1]
        main = 1 - b[1:N-1]
        upper = -c[1:N-2]
        ab = np.zeros((3, N - 2))
        ab[0, 1:] = upper
        ab[1, :] = main
        ab[2, :-1] = lower
        rhs = grid.copy()
        rhs_inner = a[1:N-1] * rhs[0:N-2] + (1 + b[1:N-1]) * rhs[1:N-1] + c[1:N-1] * rhs[2:N]
        grid[1:N-1] = solve_banded((1, 1), ab, rhs_inner)
    return np.interp(K, K_grid, grid)

def reprice_options(df, r=0.05):
    lv_interp = get_local_vol_interpolator(df)
    prices = []
    for _, row in df.iterrows():
        K, T, S = row["STRIKE"], row["T"], row["Cur_Price"]
        K_grid = np.linspace(0.5 * S, 1.5 * S, 200)
        p = crank_nicolson_local_vol(K, T, S, r, lv_interp, K_grid, len(K_grid), 100)
        prices.append(p)
    df["LocalVol_Price"] = prices
    df["LocalVol_Error"] = df["Market_Price"] - df["LocalVol_Price"]
    df["Abs_Error"] = np.abs(df["LocalVol_Error"])
    df["Rel_Error(%)"] = 100 * df["Abs_Error"] / df["Market_Price"]
    return df

# ---- 6. Full Pipeline ----
def full_local_vol_pipeline(df, r=0.05):
    df["Implied_Vol"] = df.apply(lambda row: newton_raphson_iv(
        row["Market_Price"], row["Cur_Price"], row["STRIKE"], row["T"], r), axis=1)
    df["log_moneyness"] = np.log(df["STRIKE"] / df["Cur_Price"])
    df["total_variance"] = df["Implied_Vol"]**2 * df["T"]
    k = df["log_moneyness"].values
    w = df["total_variance"].values
    df, svi_results = fit_svi(df, k, w)
    bf_ok = butterfly_arbitrage_check(k, df["SVI_raw_fit"].values)
    cal_ok = calendar_arbitrage_check(df)
    df = compute_local_volatility(df, svi_column="SVI_raw_fit")
    df = reprice_options(df, r)
    return df, svi_results, bf_ok, cal_ok

# ---- 7. Example Usage ----
data = {
    "Cur_Price": [15000, 15000, 15000],
    "STRIKE": [14800, 15000, 15200],
    "T": [0.1, 0.1, 0.1],
    "Market_Price": [250, 160, 90],
}
df = pd.DataFrame(data)
df, svi_params, bf_check, cal_check = full_local_vol_pipeline(df)

# Display Results
print("Butterfly Arbitrage Free:", bf_check)
print("Calendar Arbitrage Free:", cal_check)
print(df)

from scipy.stats import norm

# Black-Scholes Call price
def bs_call_price(S, K, T, r, sigma):
    if T <= 0 or sigma <= 0:
        return max(S - K, 0)
    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))
    d2 = d1 - sigma * np.sqrt(T)
    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)

# Check calendar arbitrage using SVI implied vols and Black-Scholes price
def check_calendar_arbitrage(df, r=0.0):
    results = []
    for K in sorted(df['K'].unique()):
        subset = df[df['K'] == K].copy().sort_values('T')
        prices = []
        for _, row in subset.iterrows():
            S = row['S']
            sigma = row['IV_svi']
            T = row['T']
            price = bs_call_price(S, K, T, r, sigma)
            prices.append(price)
        prices = np.array(prices)
        arbitrage = np.any(np.diff(prices) < -1e-6)
        results.append((K, arbitrage))
    return results

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# SVI total variance function
def svi_total_variance(k, a, b, rho, m, sigma):
    return a + b * (rho * (k - m) + np.sqrt((k - m)**2 + sigma**2))

# Check butterfly arbitrage: convexity of total variance w.r.t log-moneyness
def check_butterfly_arbitrage(df, spot):
    results = []
    for T in sorted(df['T'].unique()):
        subset = df[df['T'] == T].copy()
        subset['k'] = np.log(subset['K'] / spot)
        
        # Calculate total variance using SVI parameters
        row = subset.iloc[0]  # Same params for all strikes at same T
        a, b, rho, m, sigma = row['a'], row['b'], row['rho'], row['m'], row['sigma']
        subset['w'] = svi_total_variance(subset['k'], a, b, rho, m, sigma)
        
        # Numerical second derivative of w(k)
        subset = subset.sort_values('k')
        w = subset['w'].values
        k = subset['k'].values
        d2w = np.gradient(np.gradient(w, k), k)
        subset['d2w'] = d2w
        
        if np.any(d2w < 0):
            results.append((T, True))  # arbitrage exists
        else:
            results.append((T, False))
    return results

# Check calendar arbitrage: total variance should increase with T for fixed K
def check_calendar_arbitrage(df, spot):
    results = []
    strikes = sorted(df['K'].unique())
    
    for K in strikes:
        subset = df[df['K'] == K].copy()
        subset = subset.sort_values('T')
        k = np.log(K / spot)

        w = []
        for _, row in subset.iterrows():
            a, b, rho, m, sigma = row['a'], row['b'], row['rho'], row['m'], row['sigma']
            w.append(svi_total_variance(k, a, b, rho, m, sigma))
        
        if any(np.diff(w) < 0):
            results.append((K, True))  # arbitrage exists
        else:
            results.append((K, False))
    return results

# Assuming df is your DataFrame and spot is the spot price
spot = df['S'].iloc[0]  # If constant

butterfly_arb = check_butterfly_arbitrage(df, spot)
calendar_arb = check_calendar_arbitrage(df, spot)

print("Butterfly Arbitrage Found at:")
for T, is_arbitrage in butterfly_arb:
    if is_arbitrage:
        print(f"  T = {T}")

print("\nCalendar Arbitrage Found at:")
for K, is_arbitrage in calendar_arb:
    if is_arbitrage:
        print(f"  K = {K}")



import re
import fitz  # PyMuPDF

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def extract_id_text(full_text, chapter_title, subchapter_title, target_id):
    # Extract chapter text
    chapter_pattern = rf"{re.escape(chapter_title)}.*?(?=Chapter \d+:|$)"
    chapter_match = re.search(chapter_pattern, full_text, re.DOTALL | re.IGNORECASE)
    if not chapter_match:
        return f"Chapter '{chapter_title}' not found."

    chapter_text = chapter_match.group(0)

    # Extract subchapter text
    subchapter_pattern = rf"{re.escape(subchapter_title)}.*?(?=\n\d+\.\d+|Chapter \d+:|$)"
    subchapter_match = re.search(subchapter_pattern, chapter_text, re.DOTALL | re.IGNORECASE)
    if not subchapter_match:
        return f"Subchapter '{subchapter_title}' not found."

    subchapter_text = subchapter_match.group(0)

    # Use non-greedy match between target ID and next ID
    id_pattern = rf"({re.escape(target_id)}.*?)\n(?:ID\d+:|\Z)"
    id_match = re.search(id_pattern, subchapter_text, re.DOTALL | re.IGNORECASE)
    if not id_match:
        return f"ID '{target_id}' not found or couldn't extract properly."

    return id_match.group(1).strip()

# Example usage:
pdf_path = "sample.pdf"
chapter = "Chapter 2: System Design"
subchapter = "2.1 Architecture Overview"
target_id = "ID102"

full_text = extract_text_from_pdf(pdf_path)
result = extract_id_text(full_text, chapter, subchapter, target_id)

print(result)


import pandas as pd

# Clean \n and whitespace
df = df.applymap(lambda x: str(x).replace('\n', ' ').strip() if pd.notna(x) else x)

# Ensure there is a column identifying the model ID
# If not already named, you may need to rename it like:
# df.columns = ['Model ID', 'KPI', ..., 'Status']  # Adjust as needed

# Get the last column name (where colors are stored)
last_col = df.columns[-1]

# Make sure 'Model ID' column exists
model_col = 'Model ID'  # change this to your actual model identifier column name

# Group by Model ID and count colors
color_counts = []

for model_id, group in df.groupby(model_col):
    values = group[last_col].str.lower().fillna('')
    green = values.str.count(r'\bgreen\b').sum()
    yellow = values.str.count(r'\byellow\b').sum()
    red = values.str.count(r'\bred\b').sum()
    
    color_counts.append({
        "Model ID": model_id,
        "Green": int(green),
        "Yellow": int(yellow),
        "Red": int(red)
    })

# Create a new summary DataFrame
summary_df = pd.DataFrame(color_counts)

# Output
print(summary_df)
# Optionally export
# summary_df.to_excel("model_color_counts.xlsx", index=False)

import tabula
import pandas as pd

# Step 1: Read the table from PDF image (assuming you have it as a PDF)
# If you're starting from image (like your photo), convert to PDF first
# tables = tabula.read_pdf("converted.pdf", pages='all', multiple_tables=False, lattice=True)
tables = tabula.read_pdf("kpi_table.pdf", pages='all', multiple_tables=False)

# Step 2: Take the first table
df = tables[0]

# Step 3: Clean newlines
df = df.applymap(lambda x: x.replace('\n', ' ').strip() if isinstance(x, str) else x)

# Step 4: Merge rows where first column is blank (KPI #)
merged_rows = []
prev_row = None

for _, row in df.iterrows():
    if pd.isna(row[0]) or str(row[0]).strip() == '':
        if prev_row is not None:
            prev_row = [
                (prev if pd.isna(curr) or str(curr).strip() == '' else f"{str(prev)} {str(curr)}")
                for prev, curr in zip(prev_row, row)
            ]
    else:
        if prev_row is not None:
            merged_rows.append(prev_row)
        prev_row = list(row)

if prev_row is not None:
    merged_rows.append(prev_row)

# Step 5: Create cleaned DataFrame
clean_df = pd.DataFrame(merged_rows, columns=df.columns)

# Step 6: Save or display
clean_df.to_excel("cleaned_kpi_table.xlsx", index=False)
print(clean_df)


# Assuming the first column is key and should never be blank
merged_rows = []
prev_row = None

for _, row in df.iterrows():
    if pd.isna(row.iloc[0]) or str(row.iloc[0]).strip() == '':
        if prev_row is not None:
            prev_row = [
                (prev if pd.isna(val) or str(val).strip() == '' else str(prev) + ' ' + str(val))
                for prev, val in zip(prev_row, row)
            ]
    else:
        if prev_row is not None:
            merged_rows.append(prev_row)
        prev_row = list(row)

if prev_row is not None:
    merged_rows.append(prev_row)

# Convert to DataFrame
cleaned_df = pd.DataFrame(merged_rows, columns=df.columns)

print(cleaned_df)

import re
import pdfplumber
import camelot

def find_toc_page(pdf_path, max_pages=5):
    with pdfplumber.open(pdf_path) as pdf:
        for i in range(min(max_pages, len(pdf.pages))):
            text = pdf.pages[i].extract_text()
            if text and ("contents" in text.lower() or "chapter" in text.lower()):
                return i + 1  # return 1-based page index
    return None

def extract_page_number_from_toc(pdf_path, subchapter_title, max_pages=5):
    toc_page = find_toc_page(pdf_path, max_pages)
    if toc_page is None:
        print("Table of Contents not found.")
        return None

    with pdfplumber.open(pdf_path) as pdf:
        text = pdf.pages[toc_page - 1].extract_text()
        lines = text.split('\n')

        for line in lines:
            if subchapter_title.lower() in line.lower():
                match = re.search(r'(\d+)\s*$', line.strip())  # extract ending number
                if match:
                    return int(match.group(1))
    print(f"Subchapter '{subchapter_title}' not found in ToC.")
    return None

def extract_tables_from_subchapter(pdf_path, subchapter_title, offset_pages=2):
    page_num = extract_page_number_from_toc(pdf_path, subchapter_title)
    if not page_num:
        return []

    pages = list(range(page_num, page_num + offset_pages + 1))  # include following pages
    page_str = ",".join(map(str, pages))
    tables = camelot.read_pdf(pdf_path, pages=page_str, strip_text='\n')
    return [table.df for table in tables]

# Example usage
pdf_path = "your_file.pdf"
subchapter = "2.1 Revenue Analysis"

tables = extract_tables_from_subchapter(pdf_path, subchapter)

if tables:
    print(f"Extracted {len(tables)} tables.")
    print(tables[0])  # Show first table
else:
    print("No tables found.")

import camelot
import pandas as pd

# Extract tables
tables = camelot.read_pdf("file.pdf", pages="1", flavor="stream")

# Get the first table
df = tables[0].df

# Clean newlines
df = df.applymap(lambda x: x.replace('\n', ' ').strip() if isinstance(x, str) else x)

# Optionally set first row as header
df.columns = df.iloc[0]
df = df[1:].reset_index(drop=True)

# Handle merged rows (assumes first column should not be empty)
merged_rows = []
prev_row = None

for _, row in df.iterrows():
    if row.iloc[0].strip() == '':  # Assume first column (e.g., ID) is the key
        # Merge into previous row
        if prev_row is not None:
            prev_row = [
                (prev if val.strip() == '' else prev + ' ' + val)
                for prev, val in zip(prev_row, row)
            ]
    else:
        if prev_row is not None:
            merged_rows.append(prev_row)
        prev_row = list(row)

# Append the last processed row
if prev_row is not None:
    merged_rows.append(prev_row)

# Convert to DataFrame
cleaned_df = pd.DataFrame(merged_rows, columns=df.columns)

print(cleaned_df)

import os
import shutil

def create_folders_and_copy_pdf(base_path, folder_dict, pdf_path):
    if not os.path.isfile(pdf_path):
        print(f"[ERROR] PDF file does not exist at: {pdf_path}")
        return

    pdf_filename = os.path.basename(pdf_path)
    print(f"[INFO] Starting folder creation and PDF copying...")
    print(f"[INFO] PDF to copy: {pdf_filename}\n")

    for level1 in folder_dict:
        path1 = os.path.join(base_path, level1)
        os.makedirs(path1, exist_ok=True)
        print(f"[CREATE] Level 1 folder: {path1}")

        for level2 in folder_dict[level1]:
            path2 = os.path.join(path1, level2)
            os.makedirs(path2, exist_ok=True)
            print(f"[CREATE] â””â”€â”€ Level 2 folder: {path2}")

            for level3 in folder_dict[level1][level2]:
                path3 = os.path.join(path2, level3)
                os.makedirs(path3, exist_ok=True)
                print(f"[CREATE]     â””â”€â”€ Level 3 folder: {path3}")

                # Copy PDF to this folder
                dest_pdf_path = os.path.join(path3, pdf_filename)
                shutil.copy2(pdf_path, dest_pdf_path)
                print(f"[COPY]         â””â”€â”€ PDF copied to: {dest_pdf_path}")

    print("\nâœ… All folders created and PDF copied successfully.\n")

# === Example usage ===
folder_structure = {
    'A': {
        'A1': ['A1a', 'A1b'],
        'A2': ['A2a']
    },
    'B': {
        'B1': ['B1a']
    }
}

base_output_path = "/path/to/create/folders"    # ðŸ” Replace this with your target base directory
pdf_source_path = "/path/to/your/file.pdf"      # ðŸ” Replace this with the actual PDF file

create_folders_and_copy_pdf(base_output_path, folder_structure, pdf_source_path)

import os

def extract_folder_structure(master_path):
    folder_structure = {}

    for level1 in os.listdir(master_path):
        path1 = os.path.join(master_path, level1)
        if os.path.isdir(path1):
            folder_structure[level1] = {}
            
            for level2 in os.listdir(path1):
                path2 = os.path.join(path1, level2)
                if os.path.isdir(path2):
                    folder_structure[level1][level2] = []
                    
                    for level3 in os.listdir(path2):
                        path3 = os.path.join(path2, level3)
                        if os.path.isdir(path3):
                            folder_structure[level1][level2].append(level3)

    return folder_structure

# Example usage
master_folder_path = "/path/to/master/folder"
structure = extract_folder_structure(master_folder_path)

# Print the structure clearly
import pprint
pprint.pprint(structure)

import os
import shutil

def create_folders_and_copy_pdf(base_path, folder_dict, pdf_path):
    for level1 in folder_dict:
        path1 = os.path.join(base_path, level1)
        os.makedirs(path1, exist_ok=True)

        for level2 in folder_dict[level1]:
            path2 = os.path.join(path1, level2)
            os.makedirs(path2, exist_ok=True)

            for level3 in folder_dict[level1][level2]:
                path3 = os.path.join(path2, level3)
                os.makedirs(path3, exist_ok=True)

                # Copy PDF into level3 folder
                pdf_filename = os.path.basename(pdf_path)
                dest_pdf_path = os.path.join(path3, pdf_filename)
                shutil.copy2(pdf_path, dest_pdf_path)

    print("Folder structure created and PDF copied.")

# === Example usage ===
# Folder dictionary (you can reuse output from the previous function)
folder_structure = {
    'A': {
        'A1': ['A1a', 'A1b'],
        'A2': ['A2a']
    },
    'B': {
        'B1': ['B1a']
    }
}

base_output_path = "/path/to/create/folders"    # Replace this with the base output path
pdf_source_path = "/path/to/your/file.pdf"      # Replace this with the actual PDF path

create_folders_and_copy_pdf(base_output_path, folder_structure, pdf_source_path)

import numpy as np
import pandas as pd
from scipy.interpolate import interp2d
import matplotlib.pyplot as plt

# Local vol surface as a 2D array over strikes (K) and maturities (T)
def local_vol_surface(strikes, maturities, local_vol_values):
    return interp2d(strikes, maturities, local_vol_values, kind='linear')

# Dupire PDE: finite difference for call option pricing
def price_option_dupire(S0, K, T, r, local_vol_func, S_grid_size=200, T_steps=100):
    S_max = 2 * S0
    dS = S_max / S_grid_size
    dt = T / T_steps

    S = np.linspace(0.001, S_max, S_grid_size)
    V = np.maximum(S - K, 0)  # Final payoff

    for j in range(T_steps):
        t = T - j * dt
        sigma = local_vol_func(K, t)

        # Finite difference coefficients
        A = np.zeros(S_grid_size)
        B = np.zeros(S_grid_size)
        C = np.zeros(S_grid_size)

        for i in range(1, S_grid_size - 1):
            delta = (V[i+1] - V[i-1]) / (2 * dS)
            gamma = (V[i+1] - 2*V[i] + V[i-1]) / (dS**2)

            vol_sq = sigma**2
            A[i] = 0.5 * vol_sq * S[i]**2 * gamma
            B[i] = r * S[i] * delta
            C[i] = -r * V[i]

        V[1:-1] += dt * (A[1:-1] + B[1:-1] + C[1:-1])
        V[0] = 0
        V[-1] = S_max - K * np.exp(-r * (T - t))

    return np.interp(S0, S, V)

# Evaluation function
def evaluate_local_volatility(S0, r, strikes, maturities, local_vol_values, market_prices):
    lv_func = local_vol_surface(strikes, maturities, local_vol_values)

    results = []
    for K, T, C_market in zip(strikes, maturities, market_prices):
        C_model = price_option_dupire(S0, K, T, r, lv_func)
        error = abs(C_market - C_model)
        results.append((K, T, C_market, C_model, error))

    df_results = pd.DataFrame(results, columns=["Strike", "Maturity", "Market_Price", "Model_Price", "Abs_Error"])
    return df_results

import math
from scipy.stats import norm
from scipy.optimize import brentq

class BlackScholesModel:
    def __init__(self, S, K, T, r):
        """
        S: Spot price
        K: Strike price
        T: Time to maturity (in years)
        r: Risk-free interest rate
        """
        self.S = S
        self.K = K
        self.T = T
        self.r = r

    def call_price(self, sigma):
        """Black-Scholes formula for a European call option."""
        d1 = (math.log(self.S / self.K) + (self.r + 0.5 * sigma ** 2) * self.T) / (sigma * math.sqrt(self.T))
        d2 = d1 - sigma * math.sqrt(self.T)
        return self.S * norm.cdf(d1) - self.K * math.exp(-self.r * self.T) * norm.cdf(d2)

    def put_price(self, sigma):
        """Black-Scholes formula for a European put option."""
        d1 = (math.log(self.S / self.K) + (self.r + 0.5 * sigma ** 2) * self.T) / (sigma * math.sqrt(self.T))
        d2 = d1 - sigma * math.sqrt(self.T)
        return self.K * math.exp(-self.r * self.T) * norm.cdf(-d2) - self.S * norm.cdf(-d1)

class ImpliedVolatilityCalculator:
    def __init__(self, bs_model, option_type='call'):
        """
        bs_model: Instance of BlackScholesModel
        option_type: 'call' or 'put'
        """
        self.bs_model = bs_model
        self.option_type = option_type

    def implied_volatility(self, market_price, sigma_lower=1e-6, sigma_upper=5.0, tol=1e-6):
        """
        Calculate implied volatility using Brent's method.
        market_price: Observed market price of the option
        """
        if self.option_type == 'call':
            price_fn = self.bs_model.call_price
        elif self.option_type == 'put':
            price_fn = self.bs_model.put_price
        else:
            raise ValueError("option_type must be 'call' or 'put'")

        def objective(sigma):
            return price_fn(sigma) - market_price

        try:
            return brentq(objective, sigma_lower, sigma_upper, xtol=tol)
        except ValueError:
            return float('nan')  # No solution found

# Example usage
if __name__ == "__main__":
    S = 100       # Spot price
    K = 100       # Strike price
    T = 1.0       # Time to maturity
    r = 0.05      # Risk-free interest rate
    C_market = 10 # Market price of call option

    bs = BlackScholesModel(S, K, T, r)
    iv_calc = ImpliedVolatilityCalculator(bs, option_type='call')
    iv = iv_calc.implied_volatility(C_market)
    print(f"Implied Volatility (Call): {iv:.6f}")

You are an expert in professional technical document generation. Your task is to update a performance and monitoring report using:

1. A reference PDF document that contains the previous quarterâ€™s version of the report.
2. A table of current quarter KPI results (provided separately).
3. Detailed user instructions.

Replicate the structure, formatting, and section numbering exactly as in the original document. Do not invent data. Use unchanged content directly from the original PDF where indicated, and update only the specific parts mentioned. Your writing must be formal, consistent, and clearly organized.
You are generating an updated version of a model performance report using:
- A PDF report from a previous quarter (already uploaded)
- A current quarter KPI table (provided separately)

The document must follow the original section structure precisely and contain fully updated or copied content where needed.

---

**1. Introduction**

**1.1 Executive Summary**  
Write a concise but complete executive summary based on the updated content of the entire document. Include highlights from the KPI results, key trends, and overall model performance. The summary must preview all the key findings covered in Chapters 3 to 5.

**1.2 Model Stakeholders**  
Copy this section **exactly** as it appears in the uploaded PDF. No changes are required.

**1.3 Test Source**  
Copy this section from the uploaded PDF, but apply this specific change:  
[INSERT SMALL CHANGE HERE].  
Keep everything else intact, preserving original formatting and tone.

---

**2. Monitoring Plan**  
Copy the entire â€œMonitoring Planâ€ section exactly as in the uploaded PDF. Do not alter anything.

---

**3. Model Testing and Performance**  
ðŸ’¡ This chapter must be **fully written** with **tables and analysis**, not skipped or summarized.

**3.1 Key Performance Indicator Results**  
Use the provided current quarter KPI table. Reproduce this as a clearly formatted table.  
Then, write a paragraph analyzing each KPI:  
- Explain what each metric indicates  
- Mention any improvement or decline compared to previous quarter  
- Use a clear, structured format (e.g., bullets or subheadings)

**3.2 Overall Rating, Weighting Methodology, and Rationale**  
Recalculate the overall model rating based on the current KPIs.  
Use the same structure as in the PDF:  
- Include updated scores  
- Describe the weighting methodology  
- Provide rationale for the new overall score  
ðŸ’¡ This section must be fully written with all subsections covered â€” no placeholders or skips.

**3.3 KPI History**  
Create a 4-quarter KPI comparison table.  
- Use current KPI data (provided)  
- Extract the previous 3 quarters' values from the uploaded PDF  
Then, write a comparative analysis below the table:  
- Highlight trends  
- Explain consistent or diverging behaviors  
- Be specific and technical, not vague

---

**4. Attestations**  
ðŸ’¡ This chapter must be complete. Both subsections should be **copied and lightly updated**.

**4.1 Regression Testing**  
Copy from the PDF and update the date or version reference to match the current testing cycle. Do not skip this section.

**4.2 Monitoring Plan is Up-to-Date**  
Copy this section and update only the date. Do not shorten or summarize.

---

**5. Conclusion**  
Write a new conclusion summarizing:  
- Overall model health  
- Key changes in this quarterâ€™s results  
- Any issues or regressions  
- Final recommendation

ðŸ’¡ This section must reflect insights from the KPI analysis and rating sections. Be formal and analytical.

---

**References**  
Copy the â€œReferencesâ€ section **exactly** as it appears in the PDF. No changes are allowed.

---

**Final Instructions**  
- Output must include **all sections and subsections in full**  
- Use complete paragraphs, tables, and technical tone  
- Do not leave any section empty or placeholder text  
- Use headings, numbering, and formatting exactly as in the original document

import pandas as pd
import sympy as sp
import numpy as np

def _create_lv_calculators_k():
    """
    Helper to create numerical functions from symbolic expressions,
    where SVI is parameterized by strike k.
    """
    # Define symbols. k is strike, y is log-moneyness.
    k, y, a, b, rho, m, sigma = sp.symbols('k y a b rho m sigma')
    
    # SVI total variance expression is a function of strike k
    w_k_expr = a + b * (rho * (k - m) + sp.sqrt((k - m)**2 + sigma**2))

    # Calculate derivatives of w w.r.t. k
    dw_dk = sp.diff(w_k_expr, k)
    d2w_dk2 = sp.diff(dw_dk, k)

    # Use the chain rule to find derivatives w.r.t. y
    # âˆ‚w/âˆ‚y = (âˆ‚w/âˆ‚k) * (âˆ‚k/âˆ‚y) and âˆ‚k/âˆ‚y = k
    dw_dy = dw_dk * k
    # âˆ‚Â²w/âˆ‚yÂ² = kÂ² * (âˆ‚Â²w/âˆ‚kÂ²) + k * (âˆ‚w/âˆ‚k)
    d2w_dy2 = d2w_dk2 * k**2 + dw_dk * k
    
    # Denominator from the local volatility formula
    denominator_expr = (
        1 - y / w_k_expr * dw_dy +
        0.5 * d2w_dy2 +
        0.25 * (1/w_k_expr**2 - 1/w_k_expr - 0.25) * dw_dy**2
    )
    
    # Compile symbolic expressions into fast numerical functions
    w_func = sp.lambdify((k, a, b, rho, m, sigma), w_k_expr, 'numpy')
    den_func = sp.lambdify((k, y, a, b, rho, m, sigma), denominator_expr, 'numpy')
    
    return w_func, den_func

# Pre-compile the functions for efficiency
_w_calculator_k, _denominator_calculator_k = _create_lv_calculators_k()

def add_local_volatility_k(df: pd.DataFrame, S0: float, r: float) -> pd.DataFrame:
    """
    Calculates local volatility with SVI parameterized by strike k.

    The input DataFrame must contain columns: 'k' (strike), 'T' (maturity),
    and SVI parameters: 'a', 'b', 'rho', 'm', 'sigma'.
    """
    df_result = df.copy()
    
    # Extract data from DataFrame columns
    k = df_result['k']
    T = df_result['T']
    y = np.log(k / (S0 * np.exp(r * T)))
    params = [df_result[col] for col in ['a', 'b', 'rho', 'm', 'sigma']]

    # Numerator: âˆ‚w/âˆ‚T (using finite difference)
    # This captures time-dependence of w through the forward price in k(y,T)
    dT = 1e-4
    k_at_T_plus_dt = k * np.exp(r * dT) # k(y, T+dT) = k(y,T) * exp(r*dT)
    
    w_at_T = _w_calculator_k(k, *params)
    w_at_T_plus_dt = _w_calculator_k(k_at_T_plus_dt, *params)
    numerator = (w_at_T_plus_dt - w_at_T) / dT

    # Denominator calculation using the pre-compiled symbolic result
    denominator = _denominator_calculator_k(k, y, *params)
    
    # Calculate local variance, handling numerical issues
    local_variance = np.divide(numerator, denominator, 
                               out=np.full_like(numerator, np.nan), 
                               where=(denominator!=0))
    local_variance[local_variance < 0] = 0

    df_result['local_volatility'] = np.sqrt(local_variance)
    
    return df_result

# --- Example Usage ---
if __name__ == "__main__":
    # Create a sample DataFrame
    data = {
        'T': [0.5, 0.5, 0.5, 1.0, 1.0, 1.0],
        'k': [90, 100, 110, 90, 100, 110],
        'a': [0.035, 0.035, 0.035, 0.025, 0.025, 0.025],
        'b': [0.004, 0.004, 0.004, 0.005, 0.005, 0.005], # Scaled for k
        'rho': [-0.6, -0.6, -0.6, -0.5, -0.5, -0.5],
        'm': [100, 100, 100, 102, 102, 102], # m is now in strike units
        'sigma': [15, 15, 15, 18, 18, 18] # sigma is now in strike units
    }
    sample_df = pd.DataFrame(data)

    # Call the function with market data
    result_df_k = add_local_volatility_k(df=sample_df, S0=100.0, r=0.01)

    print(result_df_k)

import pandas as pd
import sympy as sp
import numpy as np

def create_local_vol_function():
    y, T, a, b, rho, m, sigma = sp.symbols('y T a b rho m sigma')
    w_expr = a + b * (rho * (y - m) + sp.sqrt((y - m)**2 + sigma**2))
    dw_dy = sp.diff(w_expr, y)
    d2w_dy2 = sp.diff(dw_dy, y)
    den_expr = (1 - y / w_expr * dw_dy + 1/2 * d2w_dy2 + 1/4 * (1/w_expr**2 - 1/w_expr - 1/4) * dw_dy**2)
    den_func = sp.lambdify((y, a, b, rho, m, sigma), den_expr, 'numpy')
    w_func = sp.lambdify((y, a, b, rho, m, sigma), w_expr, 'numpy')
    return w_func, den_func

def calculate_local_volatility(df):
    df['y'] = np.log(df['k'] / (df['S'] * np.exp(df['r'] * df['T'])))
    w_func, den_func = create_local_vol_function()
    y_vals, a_vals, b_vals, rho_vals, m_vals, sigma_vals, T_vals = df['y'], df['a'], df['b'], df['rho'], df['m'], df['sigma'], df['T']
    w_t = w_func(y_vals, a_vals, b_vals, rho_vals, m_vals, sigma_vals)
    denominator = den_func(y_vals, a_vals, b_vals, rho_vals, m_vals, sigma_vals)
    dT = 0.001
    w_t_plus_dt = w_func(y_vals, a_vals, b_vals, rho_vals, m_vals, sigma_vals)
    total_variance_t = w_t * T_vals
    total_variance_t_plus_dt = w_t_plus_dt * (T_vals + dT)
    numerator = (total_variance_t_plus_dt - total_variance_t) / dT
    local_variance = np.full_like(numerator, np.nan)
    valid_mask = (denominator != 0)
    local_variance[valid_mask] = numerator[valid_mask] / denominator[valid_mask]
    local_variance[local_variance < 0] = 0
    df['local_volatility'] = np.sqrt(local_variance)
    return df

# Example usage
if __name__ == "__main__":
    data = {
        'T': [0.5, 0.5, 0.5, 1.0, 1.0, 1.0],
        'k': [90, 100, 110, 90, 100, 110],
        'S': [100]*6,
        'r': [0.01]*6,
        'a': [0.035, 0.035, 0.035, 0.025, 0.025, 0.025],
        'b': [0.4, 0.4, 0.4, 0.5, 0.5, 0.5],
        'rho': [-0.6, -0.6, -0.6, -0.5, -0.5, -0.5],
        'm': [0.01, 0.01, 0.01, 0.02, 0.02, 0.02],
        'sigma': [0.15, 0.15, 0.15, 0.18, 0.18, 0.18]
    }
    df = pd.DataFrame(data)
    result_df = calculate_local_volatility(df)
    print(result_df)

import pandas as pd
import sympy as sp
import numpy as np

def _create_lv_calculators():
    """Helper to create numerical functions from symbolic expressions."""
    y, a, b, rho, m, sigma = sp.symbols('y a b rho m sigma')
    
    # SVI implied variance (sigma_imp^2), denoted as w in the formula
    w_expr = a + b * (rho * (y - m) + sp.sqrt((y - m)**2 + sigma**2))
    
    # Symbolic derivatives w.r.t. log-moneyness y
    dw_dy = sp.diff(w_expr, y)
    d2w_dy2 = sp.diff(dw_dy, y)

    # Denominator from the local volatility formula
    denominator_expr = (
        1 - y / w_expr * dw_dy + 
        0.5 * d2w_dy2 +
        0.25 * (1/w_expr**2 - 1/w_expr - 0.25) * dw_dy**2
    )
    
    # Compile symbolic expressions into fast numerical functions
    w_func = sp.lambdify((y, a, b, rho, m, sigma), w_expr, 'numpy')
    den_func = sp.lambdify((y, a, b, rho, m, sigma), denominator_expr, 'numpy')
    
    return w_func, den_func

# Pre-compile the functions once for efficiency
_w_calculator, _denominator_calculator = _create_lv_calculators()

def add_local_volatility(df: pd.DataFrame, S0: float, r: float) -> pd.DataFrame:
    """
    Calculates local volatility and adds it as a column to the DataFrame.

    The input DataFrame must contain columns: 'k' (strike), 'T' (maturity),
    and SVI parameters: 'a', 'b', 'rho', 'm', 'sigma'.
    """
    df_result = df.copy()
    
    # Extract data and calculate log-moneyness y
    y = np.log(df_result['k'] / (S0 * np.exp(r * df_result['T'])))
    params = [df_result[col] for col in ['a', 'b', 'rho', 'm', 'sigma']]
    T = df_result['T']

    # Numerator: Derivative of total variance (w*T) w.r.t T (using finite difference)
    dT = 1e-4
    total_variance_t = _w_calculator(y, *params) * T
    total_variance_t_plus_dt = _w_calculator(y, *params) * (T + dT)
    numerator = (total_variance_t_plus_dt - total_variance_t) / dT

    # Denominator calculation using the pre-compiled symbolic result
    denominator = _denominator_calculator(y, *params)
    
    # Calculate local variance, handling numerical issues
    local_variance = np.divide(numerator, denominator, 
                               out=np.full_like(numerator, np.nan), 
                               where=(denominator!=0))
    local_variance[local_variance < 0] = 0

    df_result['local_volatility'] = np.sqrt(local_variance)
    
    return df_result

# --- Example Usage ---
if __name__ == "__main__":
    # Create a sample DataFrame
    data = {
        'T': [0.5, 0.5, 0.5, 1.0, 1.0, 1.0],
        'k': [90, 100, 110, 90, 100, 110],
        'a': [0.035, 0.035, 0.035, 0.025, 0.025, 0.025],
        'b': [0.4, 0.4, 0.4, 0.5, 0.5, 0.5],
        'rho': [-0.6, -0.6, -0.6, -0.5, -0.5, -0.5],
        'm': [0.01, 0.01, 0.01, 0.02, 0.02, 0.02],
        'sigma': [0.15, 0.15, 0.15, 0.18, 0.18, 0.18]
    }
    sample_df = pd.DataFrame(data)

    # Call the function with market data
    result_df = add_local_volatility(df=sample_df, S0=100.0, r=0.01)

    print(result_df)
